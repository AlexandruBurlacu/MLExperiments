{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups & Logistic Regression\n",
    "\n",
    "This notebook is a slightly enhanced demo used during my presentation during the first, closed-beta Data Science Community meetup that took place on <br> 16 <sup>th</sup> of January 2017.\n",
    "\n",
    "Here's shown how to use scikit-learn's Logistic Regression, and SGDClassifier (that is a more robust implementation of LR, mainly used for large datasets).\n",
    "\n",
    "---\n",
    "\n",
    "The used dataset is 20 Newsgroups, pre-vectorized using TF-IDF algorithm.\n",
    "\n",
    "I've used a 5-fold cross validation scheme and Grid Search for hyperparameter tuning.\n",
    "\n",
    "Also to get higher accuracy, feature vectors were scaled and normalized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing, cross_validation, datasets\n",
    "import cPickle as cpk\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_FILE_PATH = \"models/lr.model\"\n",
    "SGD_FILE_PATH = \"models/sgd.model\"\n",
    "LR_OP_FILE_PATH = \"models/optimized_lr.model\"\n",
    "SGD_OP_FILE_PATH = \"models/optimized_sgd.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 1.20432806015 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Number of documents in the dataset: 11314\n",
      "Size of the feature vector of a document: 130107\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "news = datasets.fetch_20newsgroups_vectorized()\n",
    "print(\"Data loaded in {0} seconds\".format(time() - start))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "X = news.data\n",
    "y = news.target\n",
    "\n",
    "# _ = [print(topic) for topic in news.target_names] # you may uncoment this to see the topics\n",
    "\n",
    "print(\"Number of documents in the dataset: {0}\".format(*X.shape))\n",
    "print(\"Size of the feature vector of a document: {1}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_str_repr(label):\n",
    "    return news.target_names[label]\n",
    "\n",
    "def formated_print(model_name, acc):\n",
    "    pretty_stat = \"Accuracy of the {0} model is {1}.\".format(model_name, acc)\n",
    "    print(pretty_stat)\n",
    "    \n",
    "def train_estimator(estimator, features):\n",
    "    train, _ = next(iter(kfcv))\n",
    "    estimator.fit(features[train], y[train])\n",
    "    return estimator\n",
    "\n",
    "def save_estimator(estimator, filename):\n",
    "    \"\"\"\n",
    "        Note that it is recomended to use '.pkl' or '.model' file extension.\n",
    "        \n",
    "        Why? Because I want so.\n",
    "        ;)\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        cpk.dump(estimator, fp)\n",
    "\n",
    "def load_estimator(filename):\n",
    "        with open(filename, \"rb\") as fp:\n",
    "            return cpk.load(fp)\n",
    "\n",
    "def load_or_train(estimator, feature_set, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Training...\")\n",
    "        start = time()\n",
    "        estimator = train_estimator(estimator, feature_set)\n",
    "        print(\"Trained in {0} seconds.\".format(time() - start))\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"Saving...\")\n",
    "        start = time()\n",
    "        save_estimator(estimator, filename)\n",
    "        print(\"Saved in {0} seconds.\".format(time() - start))\n",
    "    else:\n",
    "        print(\"Loading...\")\n",
    "        start = time()\n",
    "        estimator = load_estimator(filename)\n",
    "        print(\"Loaded in {0} seconds.\".format(time() - start))\n",
    "        \n",
    "    return estimator\n",
    "        \n",
    "def evaluate_estimator(estimator, features):\n",
    "    print(\"Evaluating model's accuracy...\")\n",
    "    start = time()\n",
    "    score = np.mean(cross_validation.cross_val_score(estimator, features, y, cv = kfcv, n_jobs = 4))\n",
    "    print(\"Evaluation done in {0} seconds\".format(time() - start))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature vector normalization and scaling**\n",
    "\n",
    "Due to the fact that the vectors are sparse, we must scale them just by the standard variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__X = preprocessing.scale(X, with_mean = False)\n",
    "X_new = preprocessing.normalize(__X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Fold cross validation**\n",
    "\n",
    "In order to get more objective results, I have used a 5 fold cross validation scheme, with feature-target pairs shuffling. Because of it, it is now possible to get an averaged result from 5 model evaluations, with training and testing on different portions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfcv = cross_validation.KFold(len(y), n_folds = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators with default parameters**\n",
    "\n",
    "Almost default, in order to speed up the training process I changed the number of workers from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDClassifier(n_jobs = 3)\n",
    "lr = linear_model.LogisticRegression(n_jobs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for default estimators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded in 3.42451095581 seconds.\n"
     ]
    }
   ],
   "source": [
    "lr = load_or_train(lr, X, LR_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n",
      "Evaluation done in 92.5229671001 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Accuracy of the Linear Regression model is 0.792645537933.\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(lr, X)\n",
    "\n",
    "formated_print(\"Linear Regression\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded in 4.05311703682 seconds.\n"
     ]
    }
   ],
   "source": [
    "sgd = load_or_train(sgd, X, SGD_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n",
      "Evaluation done in 3.10582184792 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Accuracy of the Stochastic Gradient Descent model is 0.867332004143.\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(sgd, X)\n",
    "\n",
    "formated_print(\"Stochastic Gradient Descent\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimized estimators**\n",
    "\n",
    "After playing around with Grid Search hyperparameter optimizer, I've found the below near optimial configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_op = linear_model.SGDClassifier(n_iter = 25, alpha = 0.00005, n_jobs = 3)\n",
    "lr_op = linear_model.LogisticRegression(max_iter = 500, C = 3593.8136638046258, n_jobs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for optimized models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded in 3.9715089798 seconds.\n"
     ]
    }
   ],
   "source": [
    "lr_op = load_or_train(lr_op, X_new, LR_OP_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(lr_op, X_new)\n",
    "\n",
    "formated_print(\"Optimized Linear Regression\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_op = load_or_train(sgd_op, X_new, SGD_OP_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_acc = evaluate_estimator(sgd_op, X_new)\n",
    "\n",
    "formated_print(\"Optimized Stochastic Gradient Descent\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From all the evaluations done above I can conclude the following 2 things:\n",
    "- Data preparation, in this case, scaling and normalizing feature vectors is extremely important and leads to major accuracy improvements.\n",
    "- SGD Classifier is much faster than Logistic Regression while the accuracy is on par or slightly better, so it is preferable when working with big datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
