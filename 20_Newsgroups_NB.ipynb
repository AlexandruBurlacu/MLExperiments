{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups & Naive Bayes\n",
    "\n",
    "This notebook is a modification of 20_Newsgroups_LR example. The motivation for this one is the great dimensionality of feature vectors in the vectorized 20 newsgroups dataset. Maybe even stronger motivation is the relatively low theoretical correlation between individual features, that in fact are term frequencies times inverse document frequency (TF-IDF).\n",
    "\n",
    "---\n",
    "\n",
    "I've used a 5-fold cross validation scheme and Parameter Sample for hyperparameter tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib gtk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import naive_bayes, preprocessing, cross_validation, datasets, grid_search\n",
    "import cPickle as cpk\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_FILE_PATH = \"models/naive_bayes.model\"\n",
    "MNB_FILE_PATH = \"models/multinomial_nb.model\"\n",
    "NB_OP_FILE_PATH = \"models/optimized_naive_bayes.model\"\n",
    "MNB_OP_FILE_PATH = \"models/optimized_multinomial_nb.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 1.73747014999 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Number of documents in the dataset: 18846\n",
      "Size of the feature vector of a document: 130107\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# remove = ('headers', 'footers', 'quotes')\n",
    "news = datasets.fetch_20newsgroups_vectorized(subset = \"all\")\n",
    "print(\"Data loaded in {0} seconds\".format(time() - start))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "X = news.data\n",
    "y = news.target\n",
    "\n",
    "# _ = [print(topic) for topic in news.target_names] # you may uncoment this to see the topics\n",
    "\n",
    "print(\"Number of documents in the dataset: {0}\".format(*X.shape))\n",
    "print(\"Size of the feature vector of a document: {1}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_str_repr(label):\n",
    "    return news.target_names[label]\n",
    "\n",
    "def formated_print(model_name, acc):\n",
    "    pretty_stat = \"Accuracy of the {0} model is {1}.\".format(model_name, acc)\n",
    "    print(pretty_stat)\n",
    "    \n",
    "def train_estimator(estimator, features):\n",
    "    train, _ = next(iter(kfcv))\n",
    "    estimator.fit(features[train], y[train])\n",
    "    return estimator\n",
    "\n",
    "def save_estimator(estimator, filename):\n",
    "    \"\"\"\n",
    "        Note that it is recomended to use '.pkl' or '.model' file extension.\n",
    "        \n",
    "        Why? Because I want so.\n",
    "        ;)\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        cpk.dump(estimator, fp)\n",
    "\n",
    "def load_estimator(filename):\n",
    "        with open(filename, \"rb\") as fp:\n",
    "            return cpk.load(fp)\n",
    "\n",
    "def load_or_train(estimator, feature_set, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Training...\")\n",
    "        start = time()\n",
    "        estimator = train_estimator(estimator, feature_set)\n",
    "        print(\"Trained in {0} seconds.\".format(time() - start))\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"Saving...\")\n",
    "        start = time()\n",
    "        save_estimator(estimator, filename)\n",
    "        print(\"Saved in {0} seconds.\".format(time() - start))\n",
    "    else:\n",
    "        print(\"Loading...\")\n",
    "        start = time()\n",
    "        estimator = load_estimator(filename)\n",
    "        print(\"Loaded in {0} seconds.\".format(time() - start))\n",
    "        \n",
    "    return estimator\n",
    "        \n",
    "def evaluate_estimator(estimator, features):\n",
    "    print(\"Evaluating model's accuracy...\")\n",
    "    start = time()\n",
    "    score = np.mean(cross_validation.cross_val_score(estimator, features, y, cv = kfcv, n_jobs = 1))\n",
    "    print(\"Evaluation done in {0} seconds\".format(time() - start))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature vector normalization and scaling**\n",
    "\n",
    "Due to the fact that the vectors are sparse, we must scale them just by the standard variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__X = preprocessing.scale(X, with_mean = False)\n",
    "X_new = preprocessing.normalize(__X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Fold cross validation**\n",
    "\n",
    "In order to get more objective results, I have used a 5 fold cross validation scheme, with feature-target pairs shuffling. Because of it, it is now possible to get an averaged result from 5 model evaluations, with training and testing on different portions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfcv = cross_validation.KFold(len(y), n_folds = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators with default parameters**\n",
    "\n",
    "Almost default, in order to speed up the training process I changed the number of workers from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaussian_nb = naive_bayes.GaussianNB()\n",
    "multinomial_nb = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for default estimators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**\n",
    "\n",
    "Due to the Memory Error I chose to use `partial_fit()` method, and train and test the model on mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gaussian_nb = load_or_train(gaussian_nb, X, NB_FILE_PATH)\n",
    "train, test = next(iter(kfcv))\n",
    "\n",
    "for batch_tr in np.split(train, 3769):\n",
    "    gaussian_nb.partial_fit(X_new[batch_tr].toarray(), y[batch_tr], classes = range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Batch Nr.1 Accuracy 0.766578249337\n",
      "Batch Nr.2 Accuracy 0.761273209549\n",
      "Batch Nr.3 Accuracy 0.771883289125\n",
      "Batch Nr.4 Accuracy 0.801061007958\n",
      "Batch Nr.5 Accuracy 0.774535809019\n",
      "Batch Nr.6 Accuracy 0.726790450928\n",
      "Batch Nr.7 Accuracy 0.753315649867\n",
      "Batch Nr.8 Accuracy 0.708222811671\n",
      "Batch Nr.9 Accuracy 0.74801061008\n",
      "Batch Nr.10 Accuracy 0.742705570292\n",
      "-------------------------------------------------------------------------------\n",
      "Evaluation done in 116.711883068 seconds\n",
      "Accuracy of the Gaussian Naive Bayes model is 0.755437665782.\n"
     ]
    }
   ],
   "source": [
    "# mean_acc = evaluate_estimator(gaussian_nb, X)\n",
    "acc = 0.0\n",
    "print(\"Evaluating...\")\n",
    "start = time()\n",
    "for i, batch in enumerate(np.split(test, 10)):\n",
    "    current_acc = gaussian_nb.score(X_new[batch].toarray(), y[batch])\n",
    "    print(\"Batch Nr.{0} Accuracy {1}\".format(i + 1, current_acc))\n",
    "    acc += current_acc\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Evaluation done in {0} seconds\".format(time() - start))\n",
    "formated_print(\"Gaussian Naive Bayes\", acc / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Trained in 0.263236999512 seconds.\n",
      "-------------------------------------------------------------------------------\n",
      "Saving...\n",
      "Saved in 6.56074094772 seconds.\n"
     ]
    }
   ],
   "source": [
    "multinomial_nb = load_or_train(multinomial_nb, X_new, MNB_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.66627597809 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Accuracy of the Multinomial Naive Bayes model is 0.889844445086.\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(multinomial_nb, X_new)\n",
    "\n",
    "formated_print(\"Multinomial Naive Bayes\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter selection**\n",
    "\n",
    "To choose near-optimal hyperparameters I will use Parameter Sample from scikit-learn.\n",
    "Also it should be mentioned that I will search just for Multinomial Naive Bayes estimator, due to it's superior performance over Gaussian Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multinomial_nb_search_space = {\"fit_prior\": [True, False],\n",
    "                               \"alpha\": [10 ** -x for x in range(5)]\n",
    "                              }\n",
    "param_list = grid_search.ParameterSampler(multinomial_nb_search_space, n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded in 8.56118392944 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.5784201622 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 8.09067201614 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.62555885315 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 9.28507590294 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 2.26200318336 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 7.15705895424 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 2.22633314133 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 7.60700011253 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.82064414024 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 7.86306619644 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.93265509605 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 8.71616697311 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 2.28045201302 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 8.83849406242 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.86912083626 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 9.54496097565 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 2.0252392292 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Loading...\n",
      "Loaded in 9.01600909233 seconds.\n",
      "Evaluating model's accuracy...\n",
      "Evaluation done in 2.11138105392 seconds\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for i, params in enumerate(param_list):\n",
    "    nb = naive_bayes.MultinomialNB(**params)\n",
    "    nb = load_or_train(nb, X_new, \"tmp/tmp{0}.model\".format(i))\n",
    "    acc = evaluate_estimator(nb, X_new)\n",
    "    results[acc] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90549746536205933, 0.90448936704780658, 0.9022077495244254, 0.89785640640911857, 0.89716665270850504, 0.88984444508565963, 0.88766837941520704, 0.8875092000706587, 0.88018671093867107, 0.87965608028077713]\n"
     ]
    }
   ],
   "source": [
    "best_results = list(reversed(sorted(results.keys())))\n",
    "xs, ys = zip(*map(lambda param: (param[\"alpha\"], param[\"fit_prior\"]), results.values()))\n",
    "\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('Blues')\n",
    "sc = plt.scatter(xs, ys, c = best_results, vmin = 0.87, vmax = .91, s = 50, cmap = cm)\n",
    "plt.colorbar(sc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimized estimators**\n",
    "\n",
    "After playing around with Parameter Sampler hyperparameter optimizer, I've found the below near optimial configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multinomial_nb_op = naive_bayes.MultinomialNB(**results[best_results[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for optimized models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Trained in 0.280801057816 seconds.\n",
      "-------------------------------------------------------------------------------\n",
      "Saving...\n",
      "Saved in 5.22973680496 seconds.\n"
     ]
    }
   ],
   "source": [
    "multinomial_nb_op = load_or_train(multinomial_nb_op, X_new, MNB_OP_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.40515398979 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Accuracy of the Optimized Multinomial Naive Bayes model is 0.905497465362.\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(multinomial_nb_op, X_new)\n",
    "\n",
    "formated_print(\"Optimized Multinomial Naive Bayes\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From all the evaluations done above I can conclude the following 2 things:\n",
    "- Data preparation, in this case, scaling and normalizing feature vectors is extremely important and leads to major accuracy improvements (Like I didn't knew it...).\n",
    "- Multinomial Naive Bayes can be trained significantly faster than SGD Classifier, but its accuracy is slower, which means that SGD Classifier is still preferable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
