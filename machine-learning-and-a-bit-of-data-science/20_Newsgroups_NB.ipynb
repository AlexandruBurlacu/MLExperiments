{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups & Naive Bayes\n",
    "\n",
    "This notebook is a modification of 20_Newsgroups_LR example. The motivation for this one is the great dimensionality of feature vectors in the vectorized 20 newsgroups dataset. Maybe even stronger motivation is the relatively low theoretical correlation between individual features, that in fact are term frequencies times inverse document frequency (TF-IDF).\n",
    "\n",
    "---\n",
    "\n",
    "I've used a 5-fold cross validation scheme and Parameter Sample for hyperparameter tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexburlacu/Documents/Projects/MLExperiments/machine-learning-and-a-bit-of-data-science/.venv/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/alexburlacu/Documents/Projects/MLExperiments/machine-learning-and-a-bit-of-data-science/.venv/local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import naive_bayes, preprocessing, cross_validation, datasets, grid_search\n",
    "import cPickle as cpk\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FILE_PATH = \"../tmp/models/naive_bayes.pkl\"\n",
    "MNB_FILE_PATH = \"../tmp/models/multinomial_nb.pkl\"\n",
    "NB_OP_FILE_PATH = \"../tmp/models/optimized_naive_bayes.pkl\"\n",
    "MNB_OP_FILE_PATH = \"../tmp/models/optimized_multinomial_nb.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 1.25855898857 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Number of documents in the dataset: 18846\n",
      "Size of the feature vector of a document: 130107\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# remove = ('headers', 'footers', 'quotes')\n",
    "news = datasets.fetch_20newsgroups_vectorized(subset = \"all\")\n",
    "print(\"Data loaded in {0} seconds\".format(time() - start))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "X = news.data\n",
    "y = news.target\n",
    "\n",
    "# _ = [print(topic) for topic in news.target_names] # you may uncoment this to see the topics\n",
    "\n",
    "print(\"Number of documents in the dataset: {0}\".format(*X.shape))\n",
    "print(\"Size of the feature vector of a document: {1}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str_repr(label):\n",
    "    return news.target_names[label]\n",
    "\n",
    "def formated_print(model_name, acc):\n",
    "    pretty_stat = \"Accuracy of the {0} model is {1}.\".format(model_name, acc)\n",
    "    print(pretty_stat)\n",
    "    \n",
    "def train_estimator(estimator, features):\n",
    "    train, _ = next(iter(kfcv))\n",
    "    estimator.fit(features[train], y[train])\n",
    "    return estimator\n",
    "\n",
    "def save_estimator(estimator, filename):\n",
    "    \"\"\"\n",
    "        Note that it is recomended to use '.pkl' or '.model' file extension.\n",
    "        \n",
    "        Why? Because I want so.\n",
    "        ;)\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        cpk.dump(estimator, fp)\n",
    "\n",
    "def load_estimator(filename):\n",
    "        with open(filename, \"rb\") as fp:\n",
    "            return cpk.load(fp)\n",
    "\n",
    "def load_or_train(estimator, feature_set, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Training...\")\n",
    "        start = time()\n",
    "        estimator = train_estimator(estimator, feature_set)\n",
    "        print(\"Trained in {0} seconds.\".format(time() - start))\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"Saving...\")\n",
    "        start = time()\n",
    "        save_estimator(estimator, filename)\n",
    "        print(\"Saved in {0} seconds.\".format(time() - start))\n",
    "    else:\n",
    "        print(\"Loading...\")\n",
    "        start = time()\n",
    "        estimator = load_estimator(filename)\n",
    "        print(\"Loaded in {0} seconds.\".format(time() - start))\n",
    "        \n",
    "    return estimator\n",
    "        \n",
    "def evaluate_estimator(estimator, features):\n",
    "    print(\"Evaluating model's accuracy...\")\n",
    "    start = time()\n",
    "    score = np.mean(cross_validation.cross_val_score(estimator, features, y, cv = kfcv, n_jobs = 1))\n",
    "    print(\"Evaluation done in {0} seconds\".format(time() - start))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature vector normalization and scaling**\n",
    "\n",
    "Due to the fact that the vectors are sparse, we must scale them just by the standard variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__X = preprocessing.scale(X, with_mean = False)\n",
    "X_new = preprocessing.normalize(__X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Fold cross validation**\n",
    "\n",
    "In order to get more objective results, I have used a 5 fold cross validation scheme, with feature-target pairs shuffling. Because of it, it is now possible to get an averaged result from 5 model evaluations, with training and testing on different portions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfcv = cross_validation.KFold(len(y), n_folds = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators with default parameters**\n",
    "\n",
    "Almost default, in order to speed up the training process I changed the number of workers from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = naive_bayes.GaussianNB()\n",
    "multinomial_nb = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for default estimators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**\n",
    "\n",
    "Due to the Memory Error I chose to use `partial_fit()` method, and train and test the model on mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_nb = load_or_train(gaussian_nb, X, NB_FILE_PATH)\n",
    "train, test = next(iter(kfcv))\n",
    "\n",
    "for batch_tr in np.split(train, 3769):\n",
    "    gaussian_nb.partial_fit(X_new[batch_tr].toarray(), y[batch_tr], classes = range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Batch Nr.1 Accuracy 0.811671087533\n",
      "Batch Nr.2 Accuracy 0.814323607427\n",
      "Batch Nr.3 Accuracy 0.779840848806\n",
      "Batch Nr.4 Accuracy 0.790450928382\n",
      "Batch Nr.5 Accuracy 0.769230769231\n",
      "Batch Nr.6 Accuracy 0.763925729443\n",
      "Batch Nr.7 Accuracy 0.718832891247\n",
      "Batch Nr.8 Accuracy 0.737400530504\n",
      "Batch Nr.9 Accuracy 0.774535809019\n",
      "Batch Nr.10 Accuracy 0.708222811671\n",
      "-------------------------------------------------------------------------------\n",
      "Evaluation done in 117.328653097 seconds\n",
      "Accuracy of the Gaussian Naive Bayes model is 0.766843501326.\n"
     ]
    }
   ],
   "source": [
    "# mean_acc = evaluate_estimator(gaussian_nb, X)\n",
    "acc = 0.0\n",
    "print(\"Evaluating...\")\n",
    "start = time()\n",
    "for i, batch in enumerate(np.split(test, 10)):\n",
    "    current_acc = gaussian_nb.score(X_new[batch].toarray(), y[batch])\n",
    "    print(\"Batch Nr.{0} Accuracy {1}\".format(i + 1, current_acc))\n",
    "    acc += current_acc\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Evaluation done in {0} seconds\".format(time() - start))\n",
    "formated_print(\"Gaussian Naive Bayes\", acc / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Trained in 0.328003168106 seconds.\n",
      "-------------------------------------------------------------------------------\n",
      "Saving...\n",
      "Saved in 3.91957116127 seconds.\n"
     ]
    }
   ],
   "source": [
    "multinomial_nb = load_or_train(multinomial_nb, X_new, MNB_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model's accuracy...\n",
      "Evaluation done in 1.81471395493 seconds\n",
      "-------------------------------------------------------------------------------\n",
      "Accuracy of the Multinomial Naive Bayes model is 0.887137973965.\n"
     ]
    }
   ],
   "source": [
    "mean_acc = evaluate_estimator(multinomial_nb, X_new)\n",
    "\n",
    "formated_print(\"Multinomial Naive Bayes\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter selection**\n",
    "\n",
    "To choose near-optimal hyperparameters I will use Parameter Sample from scikit-learn.\n",
    "Also it should be mentioned that I will search just for Multinomial Naive Bayes estimator, due to it's superior performance over Gaussian Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_search_space = {\"fit_prior\": [True, False],\n",
    "                               \"alpha\": [10 ** -x for x in range(5)]\n",
    "                              }\n",
    "param_list = grid_search.ParameterSampler(multinomial_nb_search_space, n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexburlacu/Documents/Projects/MLExperiments/machine-learning-and-a-bit-of-data-science/.venv/local/lib/python2.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultinomialNB from version pre-0.18 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.95780801773 seconds.\n",
      "Evaluating model's accuracy...\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for i, params in enumerate(param_list):\n",
    "    nb = naive_bayes.MultinomialNB(**params)\n",
    "    nb = load_or_train(nb, X_new, \"../tmp/tmp{0}.model\".format(i))\n",
    "    acc = evaluate_estimator(nb, X_new)\n",
    "    results[acc] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = list(reversed(sorted(results.keys())))\n",
    "xs, ys = zip(*map(lambda param: (param[\"alpha\"], param[\"fit_prior\"]), results.values()))\n",
    "\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('Blues')\n",
    "sc = plt.scatter(xs, ys, c = best_results, vmin = 0.87, vmax = .91, s = 50, cmap = cm)\n",
    "plt.colorbar(sc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimized estimators**\n",
    "\n",
    "After playing around with Parameter Sampler hyperparameter optimizer, I've found the below near optimial configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_op = naive_bayes.MultinomialNB(**results[best_results[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy benchmark for optimized models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_op = load_or_train(multinomial_nb_op, X_new, MNB_OP_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = evaluate_estimator(multinomial_nb_op, X_new)\n",
    "\n",
    "formated_print(\"Optimized Multinomial Naive Bayes\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From all the evaluations done above I can conclude the following 2 things:\n",
    "- Data preparation, in this case, scaling and normalizing feature vectors is extremely important and leads to major accuracy improvements (Like I didn't knew it...).\n",
    "- Multinomial Naive Bayes can be trained significantly faster than SGD Classifier, but its accuracy is lower, which means that SGD Classifier is still preferable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
